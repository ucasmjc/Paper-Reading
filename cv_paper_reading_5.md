# Semantic segmentation
## 1.ISANet:Interlaced Sparse Self-Attention for Semantic Segmentation(2019)
[论文链接](https://arxiv.org/pdf/1907.12273.pdf(%C3%A6%C2%AD%C2%A3%C3%A5%C5%93%C2%A8%C3%A9%CB%9C%E2%80%A6%C3%A8%C2%AF%C2%BB%C3%A7%C5%A1%E2%80%9E%C3%A4%C2%B8%E2%82%AC%C3%A7%C2%AF%E2%80%A1))

简化自注意力的计算，提出了一种交错稀疏自注意力，将原稠密的亲和矩阵O($N^2$)分解为两个稀疏的亲和矩阵计算，大大减小了内存/FLOPs/耗时。

具体来说，将H\*W的输入划分成m\*n个h\*w的方格(H=mh,W=nw)，先计算“长距依赖”，将每个小方格相同位置的元素拿出来，组成新的方格（大小为m\*n），计算自注意力；再计算“短距依赖”，将第一步得到的特征图恢复原形状，在计算每个h\*w方格内部的自注意力。总的来说，每个位置的元素都可以得到其他所有位置元素传播来的信息。

![Alt text](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/image/17.png)

论文中直接给出了一段简洁的pytorch代码，我对其实现方式迷惑，发现对pytorch中permute函数不熟悉，梳理如下：

>关于pytorch中的reshape和permute函数
- reshape/view:相当于将原张量按行拉直，再填入新的形状
- permute:对于高维张量，改变的规则是什么呢？
~~~
从三维角度直观理解，对一个长宽高固定的长方体，permute只是从不同角度去看他，导致长宽高发生了改变，其实还是同一个长方体，但是view会直接改变长方体的长宽高，相当于一个没变形，一个变形了
~~~
以下面6*8张量为例
~~~py
tensor([[0.5855, 0.9252, 0.7436, 0.0545, 0.1243, 0.2341, 0.4057, 0.8889],
        [0.1092, 0.4451, 0.2793, 0.1091, 0.5837, 0.2935, 0.5816, 0.9718],
        [0.9975, 0.9356, 0.9426, 0.4008, 0.3347, 0.1301, 0.1406, 0.5253],
        [0.0612, 0.1610, 0.5503, 0.5757, 0.5057, 0.8157, 0.1558, 0.5449],
        [0.8162, 0.8662, 0.8467, 0.5890, 0.9397, 0.1468, 0.9264, 0.9635],
        [0.7283, 0.6237, 0.3733, 0.4426, 0.3941, 0.9812, 0.6998, 0.7632]])
~~~
为了实现“取每个2\*2小方块的同一位置，组成3\*4方块计算自注意力”，先用reshape转为3*2*4*2
~~~py
tensor([[[[0.5855, 0.9252],
          [0.7436, 0.0545],
          [0.1243, 0.2341],
          [0.4057, 0.8889]],

         [[0.1092, 0.4451],
          [0.2793, 0.1091],
          [0.5837, 0.2935],
          [0.5816, 0.9718]]],


        [[[0.9975, 0.9356],
          [0.9426, 0.4008],
          [0.3347, 0.1301],
          [0.1406, 0.5253]],

         [[0.0612, 0.1610],
          [0.5503, 0.5757],
          [0.5057, 0.8157],
          [0.1558, 0.5449]]],


        [[[0.8162, 0.8662],
          [0.8467, 0.5890],
          [0.9397, 0.1468],
          [0.9264, 0.9635]],

         [[0.7283, 0.6237],
          [0.3733, 0.4426],
          [0.3941, 0.9812],
          [0.6998, 0.7632]]]])
~~~
可以看出是按行填充的。  
再进行permute(1,3,0,2)，转为2\*2\*3\*4形状，此时为
~~~py
tensor([[[[0.5855, 0.7436, 0.1243, 0.4057],
          [0.9975, 0.9426, 0.3347, 0.1406],
          [0.8162, 0.8467, 0.9397, 0.9264]],

         [[0.9252, 0.0545, 0.2341, 0.8889],
          [0.9356, 0.4008, 0.1301, 0.5253],
          [0.8662, 0.5890, 0.1468, 0.9635]]],


        [[[0.1092, 0.2793, 0.5837, 0.5816],
          [0.0612, 0.5503, 0.5057, 0.1558],
          [0.7283, 0.3733, 0.3941, 0.6998]],

         [[0.4451, 0.1091, 0.2935, 0.9718],
          [0.1610, 0.5757, 0.8157, 0.5449],
          [0.6237, 0.4426, 0.9812, 0.7632]]]])
~~~
这是有迹可循的。按最开始的空间立方体理解方法，我们按维度依次去看这个张量，
- 第一个维度是2个4\*2方块中的第一个2，我们固定它，从这个角度来看原张量为3\*4\*2的方块，例如，第一个这样的方块如下
  ~~~py
  [[[0.5855, 0.9252],
    [0.7436, 0.0545],
    [0.1243, 0.2341],
    [0.4057, 0.8889]],
    [[0.9975, 0.9356],
    [0.9426, 0.4008],
    [0.3347, 0.1301],      
    [0.1406, 0.5253]],      
    [[0.8162, 0.8662],
    [0.8467, 0.5890],
    [0.9397, 0.1468],
    [0.9264, 0.9635]]]
   ~~~

- 第二个维度为原4\*2方块中的2，我们固定它，看到的为3\*4方块，第一个这样的方块如下
  ~~~py
  [[0.5855, 0.7436, 0.1243, 0.4057],
    [0.9975, 0.9426, 0.3347, 0.1406],
    [0.8162, 0.8467, 0.9397, 0.9264]]
  ~~~
- 第三个和第四个维度就显然了

OK，这样分析完，实现“取每个2\*2小方块的同一位置，组成3\*4方块计算自注意力”的思路就显现出来了。

先将6\*8张量按2\*2方块划分，变为3\*2\*4\*2，这个过程没有改变数据的位置；先将3\*2中的2提到最前，相当于按行每隔2抽取一行，变为3\*4\*2；再将4\*2中的2提到最前，相当于按列每隔2列抽一列，变为3\*4。总的来说，实现了每隔两行两列取一个元素。

-----
最后一个问题，怎么将打乱的方块换回原位（即3\*2\*4\*2）？首先将3提到最前，此时剩下的第一个2\*2\*4为打乱顺序的原前两行；再将第一个2提到最前，此时的第一个2\*4为打乱顺序的原第一行，即2\*2方格中的11和12位置；再将4提到最前，相当于转置，原属于同一个方格的两个元素到了一起，得到4\*2。

因此，只需permute(2,0,3,1)即可。

## 2.Denseaspp for semantic segmentation in street scenes(2018)
[论文链接](http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf)
没啥新东西，相当于Densenet里的那些密集链接，换成不同膨胀率的空洞卷积。意义在于：得到更大感受野范围的同时，覆盖更密集的尺度范围。
![Alt text](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/image/18.png)


## 3.PVT:Pyramid vision transformer: A versatile backbone for dense prediction without convolutions(2021)
[论文链接](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.pdf)

Swins Transformer出来之前的产物。ViT只针对分类任务，为了节约显存，划分的补丁很大，因此不适用于对分辨率高要求的密集预测任务。PVT是可以适应密集预测（提出了SRA代替多头自注意力，大大减小资源消耗，因此可以使用更细粒度的输入，比如4\*4的patch）、可以输出多尺度特征图的vision transformer。

共分四个阶段，每个阶段先计算patch嵌入减小分辨率，再输入transformer编码层。其中，SRA在计算多头自注意力时，将K和V进一步压缩，可以理解为又计算了一次$R_i$大小的patch嵌入，嵌入的维度与Q一样（相当于将N\*C中的N压缩了），由此将计算量减小了$R_i^2$倍。当然这个操作会导致信息损失，但使模型可以处理更长的序列/更细粒度的输入。

![Alt text](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/image/19.png)



# 实时backbone
## FasterNet:Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks(2023)
[论文链接](https://arxiv.org/pdf/2303.03667)

本文提出，许多“高效”网络虽然纸面上降低了FLOPs，但是实际上没有显著的加速效果，比如DWConv。这是因为，实际延迟=FLOPs/FLOPS，FLOPS为每秒浮点计算数，后者与内存访问量有关，DWConv由于会提高通道数（以补偿精度），会使FLOPS显著增大。

本文提出了PConv( Partial convolution)，只对$c_p$个通道（1/4）进行k\*k卷积，其余保持不变，通道间的信息交流由级联实现。

本文提出的FasterNet中，每个block由PConv后接两个PWConv组成，其中BN和激活层只在两个PWConv之间。以保留特征多样性并实现更低的延迟。作者认为，网络中过度使用这些层可能会限制特征多样性，从而损害性能。它还会减慢整体计算速度。

![Alt text](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/image/20.png)